% ============================================================
% CLARA CITATION FILE
% Complete BibTeX entries for citing CLARA and related works
% ============================================================

% ============================================================
% MAIN PAPER CITATION (Under Review)
% ============================================================

@article{lam2025clara,
  title={CLARA: Enhancing Multimodal Sentiment Analysis via Efficient Vision-Language Fusion},
  author={Lam, Phuong and Phan, Tuoi Thi and Tran, Thien Khai},
  journal={The Visual Computer},
  year={2025},
  publisher={Springer},
  note={Under review. Submission ID: 327916a7-2e80-4286-b6c7-91175346a1e7},
  url={https://github.com/phuonglamgithub/CLARA}
}

% ============================================================
% CODE REPOSITORY CITATION
% ============================================================

@software{lam2025clara_code,
  author={Lam, Phuong and Phan, Tuoi Thi and Tran, Thien Khai},
  title={CLARA: Official Implementation - Enhancing Multimodal Sentiment Analysis via Efficient Vision-Language Fusion},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.17862924},
  url={https://github.com/phuonglamgithub/CLARA},
  license={MIT}
}

% ============================================================
% THE VISUAL COMPUTER REFERENCES (As requested by reviewers)
% ============================================================

@article{li2025ccma,
  title={CCMA: CapsNet for audio--video sentiment analysis using cross-modal attention},
  author={Li, Yan and Wang, Hui and Chen, Xiaoling},
  journal={The Visual Computer},
  year={2025},
  volume={41},
  number={3},
  pages={1609--1620},
  publisher={Springer},
  doi={10.1007/s00371-024-03265-1}
}

@article{chen2025cthfnet,
  title={CTHFNet: Contrastive translation and hierarchical fusion network for text--video--audio sentiment analysis},
  author={Chen, Zhiyuan and Liu, Yang and Zhang, Wei},
  journal={The Visual Computer},
  year={2025},
  volume={41},
  number={7},
  pages={4405--4418},
  publisher={Springer},
  doi={10.1007/s00371-024-03398-3}
}

% ============================================================
% RELATED MULTIMODAL WORKS (Recommended by reviewers)
% ============================================================

@article{zhang2024mkeah,
  title={MKEAH: Multimodal knowledge extraction and accumulation based on hyperplane embedding for knowledge-based visual question answering},
  author={Zhang, Heng and Wang, Yue and Chen, Lin and Liu, Xin},
  journal={Virtual Reality \& Intelligent Hardware},
  volume={6},
  number={4},
  pages={280--291},
  year={2024},
  publisher={Elsevier}
}

@article{machuca2025exploring,
  title={Exploring the impact of multimodal long conversations in VR on attitudes toward behavior change, memory retention, and cognitive load},
  author={Machuca, Brittany and Li, Jiawen and Maswadi, Samar and Zack, Jessica and Czarnuch, Stephen and Lee, Taewoo and Maes, Pattie},
  journal={Computer Animation and Virtual Worlds},
  volume={36},
  number={3},
  pages={e70023},
  year={2025},
  publisher={Wiley}
}

@article{guo2023bagfn,
  title={BaGFN: Broad attentive graph fusion network for high-order feature interactions},
  author={Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={8},
  pages={4499--4513},
  year={2023},
  publisher={IEEE}
}

% ============================================================
% DATASETS
% ============================================================

@inproceedings{niu2016mvsa,
  title={Sentiment analysis on multi-view social data},
  author={Niu, Tong and Zhu, Shixia and Pang, Lei and El-Saddik, Abdulmotaleb},
  booktitle={Multimedia Modeling: 22nd International Conference, MMM 2016},
  pages={15--27},
  year={2016},
  organization={Springer},
  address={Miami, FL, USA}
}

@inproceedings{kiela2020hateful,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2611--2624},
  year={2020}
}

% ============================================================
% FOUNDATION MODELS
% ============================================================

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{he2021deberta,
  title={DeBERTa: Decoding-enhanced BERT with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{dosovitskiy2021vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

% ============================================================
% PARAMETER-EFFICIENT FINE-TUNING
% ============================================================

@inproceedings{hu2022lora,
  title={LoRA: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4582--4597},
  year={2021}
}

% ============================================================
% MULTIMODAL SENTIMENT ANALYSIS
% ============================================================

@article{bayoudh2021survey,
  title={A survey on deep multimodal learning for computer vision: Advances, trends, applications, and datasets},
  author={Bayoudh, Khaled and Knani, Raja and Hamdaoui, Fayrouz and Mtibaa, Abdellatif},
  journal={The Visual Computer},
  volume={38},
  number={8},
  pages={2939--2970},
  year={2022},
  publisher={Springer}
}

@article{hegde2025images,
  title={Do images really do the talking?},
  author={Hegde, Sanjana U and Srinivasa, K and Gupta, Aditya},
  journal={Advances in Computational Intelligence},
  volume={5},
  number={1},
  pages={1--15},
  year={2025},
  publisher={Springer}
}

@article{shao2025transformer,
  title={Transformer-based short-term memory attention for enhanced multimodal sentiment analysis},
  author={Shao, Dingxin and Xiong, Yuanjie and Zhao, Yue and Huang, Qingming and Qiao, Yu and Lin, Dahua},
  journal={The Visual Computer},
  volume={41},
  number={11},
  pages={8537--8552},
  year={2025},
  publisher={Springer}
}

@article{an2023integrating,
  title={Integrating color cues to improve multimodal sentiment analysis in social media},
  author={An, Jiarui and Wan Zainon, Wan Mohd Nazmee},
  journal={Engineering Applications of Artificial Intelligence},
  volume={126},
  pages={106874},
  year={2023},
  publisher={Elsevier}
}

@article{yang2021image,
  title={Image-text multimodal emotion classification via multi-view attentional network},
  author={Yang, Xin and Feng, Shaowu and Zhang, Yongqiang and Wang, Di},
  journal={IEEE Transactions on Multimedia},
  volume={23},
  pages={4014--4026},
  year={2021},
  publisher={IEEE}
}

@article{wang2023multimodal,
  title={Multimodal sentiment analysis via contrastive learning and adaptive modality selection},
  author={Wang, Hao and Feng, Yu and Zhang, Xiaoming and Liang, Yujie and Xu, Liang},
  journal={Sensors},
  volume={23},
  number={5},
  pages={2679},
  year={2023},
  publisher={MDPI}
}

@article{xu2022d2r,
  title={Disentangled representation learning for multimodal sentiment analysis},
  author={Xu, Nan and Mao, Wenji and Chen, Guandan},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={5492--5501},
  year={2022}
}

% ============================================================
% CROSS-MODAL ATTENTION & FUSION
% ============================================================

@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6558--6569},
  year={2019}
}

@inproceedings{lu2019vilbert,
  title={ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Aniruddha and Joty, Shafiq and Xiong, Caiming and Hoi, Steven CH},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@inproceedings{zadeh2017tensor,
  title={Tensor fusion network for multimodal sentiment analysis},
  author={Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={1103--1114},
  year={2017}
}

% ============================================================
% BASELINE METHODS
% ============================================================

@inproceedings{cai2015convolutional,
  title={Convolutional neural networks for multimedia sentiment analysis},
  author={Cai, Guoyong and Xia, Binbin},
  booktitle={Natural Language Processing and Chinese Computing},
  pages={159--167},
  year={2015},
  organization={Springer}
}

@article{poria2017review,
  title={A review of affective computing: From unimodal analysis to multimodal fusion},
  author={Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
  journal={Information Fusion},
  volume={37},
  pages={98--125},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@inproceedings{he2016resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{xu2017multisenti,
  title={MultiSentiNet: A deep semantic network for multimodal sentiment analysis},
  author={Xu, Nan and Mao, Wenji},
  booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages={2399--2402},
  year={2017}
}

@article{yu2021clmlf,
  title={Cross-modal local-global memory fusion network for multimodal sentiment analysis},
  author={Yu, Jiayi and Jiang, Jing},
  journal={Information Fusion},
  volume={76},
  pages={168--183},
  year={2021},
  publisher={Elsevier}
}

@article{xu2022cignn,
  title={Cross-modal interactive graph network for multimodal sentiment analysis},
  author={Xu, Nan and Mao, Wenji and Chen, Guandan},
  journal={IEEE Transactions on Cybernetics},
  volume={52},
  number={10},
  pages={10921--10933},
  year={2022},
  publisher={IEEE}
}

% ============================================================
% ADDITIONAL REFERENCES
% ============================================================

@article{truong2019vistanet,
  title={VistaNet: Visual aspect attention network for multimodal sentiment analysis},
  author={Truong, Quoc Tuan and Lauw, Hady W},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={305--312},
  year={2019}
}

@article{zhu2023multimodal,
  title={Multimodal sentiment analysis with image-text interaction network},
  author={Zhu, Tianrui and Li, Liang and Yang, Jun and Zhao, Shi and Liu, Haoyu and Qian, Jing},
  journal={IEEE Transactions on Multimedia},
  volume={25},
  pages={3375--3385},
  year={2023},
  publisher={IEEE}
}

@article{yu2016visual,
  title={Visual and textual sentiment analysis of a microblog using deep convolutional neural networks},
  author={Yu, Yuheng and Lin, Hongfei and Meng, Jiana and Zhao, Xiaochi},
  journal={Algorithms},
  volume={9},
  number={2},
  pages={41},
  year={2016},
  publisher={MDPI}
}

@inproceedings{cai2019multimodal,
  title={Multi-modal sarcasm detection in Twitter with hierarchical fusion model},
  author={Cai, Yitao and Cai, Huiyu and Wan, Xiaojun},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2506--2515},
  year={2019}
}

@inproceedings{yang2023fewshot,
  title={Few-shot multimodal sentiment analysis based on multimodal probabilistic fusion prompts},
  author={Yang, Jingjing and Tao, Mingyi and Zhang, Xiaoming and Li, Yong},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6045--6053},
  year={2023}
}

% ============================================================
% END OF CITATION FILE
% ============================================================
