% ==============================================================================
% CITATION FOR CLARA PAPER AND CODE
% ==============================================================================
% Please cite this work if you use CLARA in your research

% ------------------------------------------------------------------------------
% PAPER CITATION (Under Review at The Visual Computer)
% ------------------------------------------------------------------------------
@article{lam2025clara,
  title={CLARA: Enhancing Multimodal Sentiment Analysis via Efficient Vision-Language Fusion},
  author={Lam, Phuong and Phan, Tuoi Thi and Tran, Thien Khai},
  journal={The Visual Computer},
  year={2025},
  publisher={Springer},
  note={Under review. Submission ID: 327916a7-2e80-4286-b6c7-91175346a1e7},
  url={https://github.com/phuonglamgithub/CLARA}
}

% ------------------------------------------------------------------------------  
% CODE REPOSITORY CITATION (Zenodo Archive)
% ------------------------------------------------------------------------------
@software{lam2025clara_code,
  author={Lam, Phuong and Phan, Tuoi Thi and Tran, Thien Khai},
  title={CLARA: Official Implementation - Parameter-Efficient Vision-Language Fusion for Multimodal Sentiment Analysis},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.XXXXXX},
  url={https://github.com/phuonglamgithub/CLARA},
  note={Source code for paper under review at The Visual Computer}
}

% ==============================================================================
% RELATED WORK CITATIONS
% ==============================================================================

% ------------------------------------------------------------------------------
% LoRA (Low-Rank Adaptation)
% ------------------------------------------------------------------------------
@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

% ------------------------------------------------------------------------------
% CLIP (Contrastive Language-Image Pre-training)
% ------------------------------------------------------------------------------
@inproceedings{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

% ------------------------------------------------------------------------------
% DeBERTa
% ------------------------------------------------------------------------------
@article{he2020deberta,
  title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

% ------------------------------------------------------------------------------
% MVSA Datasets
% ------------------------------------------------------------------------------
@article{niu2016mvsa,
  title={MVSA: A Multilingual, Multimodal Dataset for Sentiment Analysis on Social Web},
  author={Niu, Teng and Zhu, Shiai and Pang, Lei and El Saddik, Abdulmotaleb},
  journal={IEEE Transactions on Affective Computing},
  year={2016}
}

% ------------------------------------------------------------------------------
% Hateful Memes Dataset
% ------------------------------------------------------------------------------
@article{kiela2020hateful,
  title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2611--2624},
  year={2020}
}

% ==============================================================================
% RECENT VISUAL COMPUTER CITATIONS (As Requested by Reviewers)
% ==============================================================================

% ------------------------------------------------------------------------------
% CCMA (2025) - Cross-Modal Attention for Audio-Video Sentiment
% ------------------------------------------------------------------------------
@article{ccma2025,
  title={CCMA: CapsNet for audio-video sentiment analysis using cross-modal attention},
  journal={The Visual Computer},
  volume={41},
  number={3},
  pages={1609--1620},
  year={2025},
  publisher={Springer}
}

% ------------------------------------------------------------------------------
% CTHFNet (2025) - Hierarchical Fusion for Multimodal Sentiment
% ------------------------------------------------------------------------------
@article{cthfnet2025,
  title={CTHFNet: contrastive translation and hierarchical fusion network for text-video-audio sentiment analysis},
  journal={The Visual Computer},
  volume={41},
  number={7},
  pages={4405--4418},
  year={2025},
  publisher={Springer}
}

% ==============================================================================
% RECOMMENDED SOTA COMPARISONS (Optional)
% ==============================================================================

% ------------------------------------------------------------------------------
% MKEAH (2024) - Multimodal Knowledge Extraction
% ------------------------------------------------------------------------------
@article{zhang2024mkeah,
  title={MKEAH: Multimodal knowledge extraction and accumulation based on hyperplane embedding for knowledge-based visual question answering},
  author={Zhang, Heng and others},
  journal={Virtual Reality \& Intelligent Hardware},
  volume={6},
  number={4},
  pages={280--291},
  year={2024}
}

% ------------------------------------------------------------------------------
% BaGFN (2021) - Graph Fusion Networks
% ------------------------------------------------------------------------------
@article{bagfn2021,
  title={BaGFN: broad attentive graph fusion network for high-order feature interactions},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={8},
  pages={4499--4513},
  year={2021}
}

% ==============================================================================
% END OF CITATIONS
% ==============================================================================
